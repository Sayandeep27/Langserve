# Import RemoteRunnable from langserve
# RemoteRunnable allows you to call a LangChain chain
# that is running remotely as an API (LangServe backend)
from langserve import RemoteRunnable


# Create a RemoteRunnable object
# This URL points to a LangServe-hosted chain
#
# http://localhost:8000          → LangServe server address
# /summarize                     → Path where the chain is exposed
# /c/N4XyA                       → Internal chain identifier generated by LangServe
#
# IMPORTANT:
# You do NOT need the original chain code here.
# You are just consuming the remote chain like a client.
chain = RemoteRunnable("http://localhost:8000/summarize/c/N4XyA")


# Call the remote chain using invoke()
#
# invoke() sends a POST request to:
#   /summarize/c/N4XyA/invoke
#
# The dictionary passed here MUST match the input schema
# of the chain running on the server.
#
# In this case, the remote chain expects:
#   { "text": "<string>" }
res = chain.invoke({
    "text": (
        "Building an LLM-based application is more complex than simply "
        "calling an API. While integrating an LLM into your project can "
        "significantly enhance its capabilities, it comes with a unique "
        "set of challenges that require careful consideration. Below, "
        "we'll break down the primary obstacles you might encounter and "
        "highlight the aspects of deployment that need attention."
    )
})


# Print the response returned by the remote chain
#
# Internally:
# - LangServe runs the chain
# - LLM processes the input
# - Output parser formats the result
# - Response is returned as normal Python data
print(res)
